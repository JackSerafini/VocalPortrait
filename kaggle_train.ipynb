{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7655818,"sourceType":"datasetVersion","datasetId":4448016}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0ec00c01-bc5f-4755-9863-5a58e48e8c37","cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:00.338180Z","iopub.execute_input":"2025-06-09T20:41:00.338472Z","iopub.status.idle":"2025-06-09T20:41:00.371195Z","shell.execute_reply.started":"2025-06-09T20:41:00.338447Z","shell.execute_reply":"2025-06-09T20:41:00.370561Z"}},"outputs":[],"execution_count":1},{"id":"e1a7e326-d5fe-4acb-9ae4-96f3544c76ce","cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import models\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nfrom torchvision.datasets import ImageFolder, DatasetFolder\nfrom torch.utils.data.dataset import random_split\nfrom PIL import Image\nfrom torchvision.transforms import v2\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:00.372063Z","iopub.execute_input":"2025-06-09T20:41:00.372315Z","iopub.status.idle":"2025-06-09T20:41:09.101190Z","shell.execute_reply.started":"2025-06-09T20:41:00.372290Z","shell.execute_reply":"2025-06-09T20:41:09.100550Z"}},"outputs":[],"execution_count":2},{"id":"034d8be9-2a29-41fb-bba4-650d0f4ed2ec","cell_type":"code","source":"import os\n\nclass VocalPortraitDataset():\n    # TODO: labels not implemented\n    def __init__(self, root_path:str, transform:None):\n        # root_path is the faces folder path\n        self.root_path = root_path\n        self.transform = transform\n        self.image_paths = self.get_image_paths()\n\n    def get_image_paths(self):\n        image_paths = []\n        \n        person_folder = os.listdir(self.root_path)\n        for person in person_folder:\n            for nationality in ['English', 'Urdu']:\n                if os.path.exists(f\"{self.root_path}/{person}/{nationality}\"):\n                    video_ids = os.listdir(f\"{self.root_path}/{person}/{nationality}\")\n                    for video_id in video_ids:\n                        image_names = os.listdir(f\"{self.root_path}/{person}/{nationality}/{video_id}\")\n                        for image_name in image_names:\n                            image_paths.append(f\"{self.root_path}/{person}/{nationality}/{video_id}/{image_name}\")\n        return image_paths\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        # label = self.labels[idx]\n        return image, idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:11.547951Z","iopub.execute_input":"2025-06-09T20:41:11.548219Z","iopub.status.idle":"2025-06-09T20:41:11.579068Z","shell.execute_reply.started":"2025-06-09T20:41:11.548197Z","shell.execute_reply":"2025-06-09T20:41:11.578365Z"}},"outputs":[],"execution_count":3},{"id":"15d259a3-f33e-4f3a-be51-795d8b6d30b6","cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass EncoderBlock(nn.Module):\n  def __init__(\n    self,\n    input_dim:tuple,\n    latent_dim:int,\n    out_channel:int=None,\n  ):\n    super().__init__()\n    assert len(input_dim) == 3, \"input_dim must be a tuple of 3 numbers (C, H, W)\"\n    \n    self.in_channel = input_dim[0]\n    self.out_channel = out_channel if out_channel is not None else self.in_channel * 2  # if not passed it is inferred from in_channel\n    self.out_h = input_dim[1] // 2\n    self.latent_dim = latent_dim\n    \n    self.downscaler = nn.Sequential(\n      nn.Conv2d(in_channels=self.in_channel, out_channels=self.out_channel, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(self.out_channel),\n      nn.ReLU(),\n      nn.Conv2d(in_channels=self.out_channel, out_channels=self.out_channel, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(self.out_channel),\n      nn.ReLU(),\n      nn.Conv2d(in_channels=self.out_channel, out_channels=self.out_channel, kernel_size=4, stride=2, padding=1),\n      nn.BatchNorm2d(self.out_channel),\n      nn.ReLU()\n    )\n    \n    self.mu = nn.Linear(self.out_channel*self.out_h*self.out_h, latent_dim)\n    self.logvar = nn.Linear(self.out_channel*self.out_h*self.out_h, latent_dim)\n\n  def forward(self, x:torch.Tensor):\n    x = self.downscaler(x)\n    mu = self.mu(x.view(x.shape[0], -1))\n    logvar = self.logvar(x.view(x.shape[0], -1))\n    return x, mu, logvar\n  \nclass DecoderBlock(nn.Module):\n  def __init__(\n    self,\n    input_dim:tuple,\n    latent_dim:int\n  ):\n    super().__init__()\n    \n    self.in_channel = input_dim[0]\n    self.out_channel = self.in_channel // 2  # if not passed it is inferred from in_channel\n    self.in_h = input_dim[1]\n    self.latent_dim = latent_dim\n    \n    self.projector = nn.Linear(latent_dim, self.in_channel*self.in_h*self.in_h)\n    \n    self.upscaler = nn.Sequential(\n      nn.ConvTranspose2d(in_channels=self.in_channel, out_channels=self.out_channel, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(self.out_channel),\n      nn.ReLU(),\n      nn.ConvTranspose2d(in_channels=self.out_channel, out_channels=self.out_channel, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(self.out_channel),\n      nn.ReLU(),\n      nn.ConvTranspose2d(in_channels=self.out_channel, out_channels=self.out_channel, kernel_size=4, stride=2, padding=1),\n      nn.BatchNorm2d(self.out_channel),\n      nn.ReLU()\n    )\n    \n  def forward(self, x:torch.Tensor, prev_generation:torch.Tensor):\n    x = self.projector(x)\n    x = x.view(x.shape[0], -1, self.in_h, self.in_h) + prev_generation\n    x = self.upscaler(x)\n    return x\n    \n    \nclass MultiVae(nn.Module):\n  def __init__(self):\n    super().__init__()\n    \n    self.encoder_1 = EncoderBlock(input_dim=(3,64,64), latent_dim=512, out_channel=64)\n    self.encoder_2 = EncoderBlock(input_dim=(64,32,32), latent_dim=256)\n    self.encoder_3 = EncoderBlock(input_dim=(128,16,16), latent_dim=128)\n    \n    self.decoder_1 = DecoderBlock(input_dim=(128,16,16), latent_dim=512)\n    self.decoder_2 = DecoderBlock(input_dim=(256,8,8), latent_dim=256)\n    self.decoder_3 = DecoderBlock(input_dim=(512,4,4), latent_dim=128)\n    \n    self.generator = nn.Sequential(\n      nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1),\n      nn.Sigmoid()\n    )\n    \n    self.residual_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n    self.residual_2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n    \n    \n  def sample_latent_vector(self, mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n  \n  def encode(self, x:torch.Tensor):\n    x_new, mu1, logvar1 = self.encoder_1(x)\n    x = self.residual_1(x)\n    x_new, mu2, logvar2 = self.encoder_2(x_new + x)\n    x = self.residual_2(x)\n    x_new, mu3, logvar3 = self.encoder_3(x_new + x)\n    \n    return mu1, mu2, mu3, logvar1, logvar2, logvar3\n  \n  def decode(self, \n    mu1:torch.Tensor,\n    mu2:torch.Tensor,\n    mu3:torch.Tensor,\n    logvar1:torch.Tensor,\n    logvar2:torch.Tensor,\n    logvar3:torch.Tensor\n  ):\n    latent_1 = self.sample_latent_vector(mu1, logvar1)\n    latent_2 = self.sample_latent_vector(mu2, logvar2)\n    latent_3 = self.sample_latent_vector(mu3, logvar3)\n    \n    x = self.decoder_3(latent_3, 0)\n    x = self.decoder_2(latent_2, x)\n    x = self.decoder_1(latent_1, x)\n    \n    return self.generator(x)\n  \n  def forward(self, x:torch.Tensor):\n    mu1, mu2, mu3, logvar1, logvar2, logvar3 = self.encode(x)\n    return self.decode(mu1, mu2, mu3, logvar1, logvar2, logvar3), mu1, mu2, mu3, logvar1, logvar2, logvar3\n  \n  def KL(self, x, mu, logvar):\n    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n  \n  def loss(self, x, recon_x, mu1, mu2, mu3, logvar1, logvar2, logvar3):\n    recon_loss = F.mse_loss(x, recon_x, reduction='sum') / x.size(0)\n    kl1 = self.KL(x, mu1, logvar1)\n    kl2 = self.KL(x, mu2, logvar2)\n    kl3 = self.KL(x, mu3, logvar3)\n    return recon_loss + kl1 + kl2 + kl3, recon_loss, kl1, kl2, kl3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:13.408402Z","iopub.execute_input":"2025-06-09T20:41:13.408680Z","iopub.status.idle":"2025-06-09T20:41:13.451905Z","shell.execute_reply.started":"2025-06-09T20:41:13.408658Z","shell.execute_reply":"2025-06-09T20:41:13.451278Z"}},"outputs":[],"execution_count":4},{"id":"8c2700de-169f-48e6-a5ec-8019dcc03a97","cell_type":"code","source":"class plainVAE(nn.Module):\n    def __init__(self, latent_dim: int = 128):\n        super(plainVAE, self).__init__()\n        self.latent_dim = latent_dim\n\n        # Encoder: input 3x128x128 -> latent mean and log variance\n        self.encoder = nn.Sequential(\n            # Layer 1: 3x128x128 -> 32x64x64 # Should I use bigger images?\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Layer 2: 32x64x64 -> 64x32x32 # Should I use bigger images?\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            # Layer 3: 64x32x32 -> 128x16x16\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n\n            # Layer 4: 128x16x16 -> 256x8x8\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n\n            # # Layer 5: 256x8x8 -> 512x4x4\n            # nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            # nn.BatchNorm2d(512),\n            # nn.ReLU(inplace=True),\n        )\n\n        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n\n        self.decoder_fc = nn.Linear(latent_dim, 512 * 4 * 4)\n        self.decoder = nn.Sequential(\n            # Layer 1: 512*4*4 -> 256*8*8\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n\n            # Layer 2: 256*8*8 -> 128*16*16\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            # Layer 3: 128*16*16 -> 64*32*32\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            # # Layer 4: 64*32*32 -> 32*64*64\n            # nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            # nn.BatchNorm2d(32),\n            # nn.ReLU(inplace=True),\n\n            # Layer 4: 32*64*64 -> 3*128*128\n            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = h.view(h.size(0), -1)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n    \n    def reparameterization(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def decode(self, z):\n        h = self.decoder_fc(z)\n        h = h.view(h.size(0), 512, 4, 4)\n        return self.decoder(h)\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterization(mu, logvar)\n        x_reconstructed = self.decode(z)\n        return x_reconstructed, mu, logvar\n\n    def loss(self, x, recon_x, mu, logvar, beta: float = 1.0):\n        recon_loss = F.mse_loss(x, recon_x, reduction='sum') / x.size(0)\n        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n\n        return recon_loss + beta * kl, recon_loss, kl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:18.927540Z","iopub.execute_input":"2025-06-09T20:41:18.927813Z","iopub.status.idle":"2025-06-09T20:41:18.962515Z","shell.execute_reply.started":"2025-06-09T20:41:18.927792Z","shell.execute_reply":"2025-06-09T20:41:18.961759Z"}},"outputs":[],"execution_count":7},{"id":"3a23432d-fc15-4222-abb2-d4bb74d995e3","cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),         # this ensures values ∈ [0,1]\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:24.141729Z","iopub.execute_input":"2025-06-09T20:41:24.142026Z","iopub.status.idle":"2025-06-09T20:41:24.172995Z","shell.execute_reply.started":"2025-06-09T20:41:24.142003Z","shell.execute_reply":"2025-06-09T20:41:24.172424Z"}},"outputs":[],"execution_count":8},{"id":"dd9cda0a-3bae-44a5-ba99-c1da7b1a4cee","cell_type":"code","source":"# VOICEPORTRAIT dataset\n\ndata_root = \"/kaggle/input/vocalportrait/mavceleb_v1_train_cropped/mavceleb_v1_train_cropped/faces\"\ndataset = VocalPortraitDataset(data_root, transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f72be01b-a590-4a64-8e74-2352a8f12919","cell_type":"code","source":"# CELEBVOICE dataset\n\ndata_root = \"/kaggle/input/cropped-celeba\"\ndataset = ImageFolder(root=data_root, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:26.886500Z","iopub.execute_input":"2025-06-09T20:41:26.887036Z","iopub.status.idle":"2025-06-09T20:46:39.510222Z","shell.execute_reply.started":"2025-06-09T20:41:26.887012Z","shell.execute_reply":"2025-06-09T20:46:39.509653Z"}},"outputs":[],"execution_count":9},{"id":"cc04b464-ef59-4f51-a527-4c8cb7f2fb26","cell_type":"code","source":"lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\ntrain_dataset, test_dataset = random_split(dataset, lengths)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    pin_memory=True,\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=64,\n    shuffle=True,\n    pin_memory=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:46:39.511361Z","iopub.execute_input":"2025-06-09T20:46:39.511593Z","iopub.status.idle":"2025-06-09T20:46:39.552458Z","shell.execute_reply.started":"2025-06-09T20:46:39.511576Z","shell.execute_reply":"2025-06-09T20:46:39.551751Z"}},"outputs":[],"execution_count":10},{"id":"3501ef09-08ac-478d-8436-316e9e029761","cell_type":"code","source":"# ----------------------------\n# 3) Instantiate model, optimizer, device\n# ----------------------------\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel = plainVAE().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:46:39.553234Z","iopub.execute_input":"2025-06-09T20:46:39.553464Z","iopub.status.idle":"2025-06-09T20:46:39.893700Z","shell.execute_reply.started":"2025-06-09T20:46:39.553448Z","shell.execute_reply":"2025-06-09T20:46:39.893065Z"}},"outputs":[],"execution_count":11},{"id":"f9e9987e-e640-465b-9bbe-46f9acc5e701","cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/working/vae_epoch5.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T15:47:11.035313Z","iopub.execute_input":"2025-06-09T15:47:11.035881Z","iopub.status.idle":"2025-06-09T15:47:11.103094Z","shell.execute_reply.started":"2025-06-09T15:47:11.035860Z","shell.execute_reply":"2025-06-09T15:47:11.102340Z"}},"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":111},{"id":"6d31e70f-42a9-4af1-8ef8-dc582331f454","cell_type":"code","source":"# ----------------------------\n# 4) Training loop\n# ----------------------------\nnum_epochs = 10\nlog_interval = 100   # how many batches between printouts\n\nresume = False\ncheckpoint_path = \"epoch{i}.pth\"  # or whichever epoch you want\n\nstart_epoch = 1\nif resume and os.path.exists(checkpoint_path):\n    model.load_state_dict(torch.load(checkpoint_path))\n    print(f\"Resumed model from {checkpoint_path}\")\n    # If you also saved the optimizer state, load it too:\n    # optimizer.load_state_dict(torch.load(\"optimizer_epoch5.pth\"))\n    start_epoch = int(checkpoint_path.split(\"epoch\")[1].split(\".\")[0]) + 1\n\nfor epoch in range(start_epoch, num_epochs + 1):\n    model.train()\n    running_recon_loss = 0.0\n    running_kl_loss = 0.0\n    running_total_loss = 0.0\n\n    for batch_idx, (imgs, _) in enumerate(train_dataloader, start=1):\n        imgs = imgs.to(device)   # shape (B, 3, 128, 128)\n        optimizer.zero_grad()\n\n        recon_imgs, mu, logvar = model(imgs)\n        loss, recon_loss, kl_loss = model.loss(imgs, recon_imgs, mu, logvar, beta=0.1 * epoch)\n        loss.backward()\n        optimizer.step()\n\n        running_total_loss += loss.item()\n        running_recon_loss += recon_loss.item()\n        running_kl_loss += kl_loss.item()\n\n        if batch_idx % log_interval == 0:\n            avg_total = running_total_loss / log_interval\n            avg_recon = running_recon_loss / log_interval\n            avg_kl    = running_kl_loss / log_interval\n            print(f\"Epoch [{epoch}/{num_epochs}]  \"\n                  f\"Batch [{batch_idx}/{len(train_dataloader)}]  \"\n                  f\"Total Loss: {avg_total:.3f}  \"\n                  f\"Reconstruction: {avg_recon:.3f}  \"\n                  f\"KL: {avg_kl:.3f}\")\n            running_total_loss = 0.0\n            running_recon_loss = 0.0\n            running_kl_loss    = 0.0\n\n    # At the end of each epoch, you can save a checkpoint:\n    checkpoint_path = f\"vae_epoch{epoch}.pth\"\n    torch.save(model.state_dict(), checkpoint_path.format({\"i\": epoch}))\n\n    # (Optional) Also: generate a few samples from N(0,I) and save to disk\n    model.eval()\n    with torch.no_grad():\n        sample_z = torch.randn(64, model.latent_dim).to(device)\n        sample_imgs = model.decode(sample_z)   # (64, 3, 128, 128)\n        # Save a grid of 64 samples as a single image\n        save_image(sample_imgs.cpu(), f\"sample_epoch{epoch}.png\", nrow=8, normalize=True)\n\nprint(\"Training finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:46:39.895037Z","iopub.execute_input":"2025-06-09T20:46:39.895227Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10]  Batch [100/2019]  Total Loss: 286.599  Reconstruction: 263.578  KL: 230.206\nEpoch [1/10]  Batch [200/2019]  Total Loss: 166.644  Reconstruction: 149.903  KL: 167.414\nEpoch [1/10]  Batch [300/2019]  Total Loss: 137.472  Reconstruction: 121.123  KL: 163.489\nEpoch [1/10]  Batch [400/2019]  Total Loss: 120.266  Reconstruction: 103.517  KL: 167.489\nEpoch [1/10]  Batch [500/2019]  Total Loss: 109.660  Reconstruction: 93.210  KL: 164.509\nEpoch [1/10]  Batch [600/2019]  Total Loss: 101.868  Reconstruction: 85.898  KL: 159.703\nEpoch [1/10]  Batch [700/2019]  Total Loss: 95.382  Reconstruction: 79.532  KL: 158.500\nEpoch [1/10]  Batch [800/2019]  Total Loss: 90.599  Reconstruction: 74.754  KL: 158.449\nEpoch [1/10]  Batch [900/2019]  Total Loss: 87.149  Reconstruction: 71.465  KL: 156.838\nEpoch [1/10]  Batch [1000/2019]  Total Loss: 82.605  Reconstruction: 67.077  KL: 155.278\nEpoch [1/10]  Batch [1100/2019]  Total Loss: 79.883  Reconstruction: 64.409  KL: 154.742\nEpoch [1/10]  Batch [1200/2019]  Total Loss: 78.323  Reconstruction: 62.866  KL: 154.572\nEpoch [1/10]  Batch [1300/2019]  Total Loss: 75.728  Reconstruction: 60.293  KL: 154.350\nEpoch [1/10]  Batch [1400/2019]  Total Loss: 75.036  Reconstruction: 59.586  KL: 154.498\nEpoch [1/10]  Batch [1500/2019]  Total Loss: 73.465  Reconstruction: 57.930  KL: 155.354\nEpoch [1/10]  Batch [1600/2019]  Total Loss: 71.459  Reconstruction: 55.967  KL: 154.922\nEpoch [1/10]  Batch [1700/2019]  Total Loss: 70.759  Reconstruction: 55.280  KL: 154.790\nEpoch [1/10]  Batch [1800/2019]  Total Loss: 69.950  Reconstruction: 54.405  KL: 155.456\n","output_type":"stream"}],"execution_count":null},{"id":"96351251-3419-41bf-955a-daaa7f4672a0","cell_type":"code","source":"model.eval()\nfor batch_idx, (imgs, _) in enumerate(test_dataloader, start=1):\n    with torch.no_grad():\n        imgs = imgs.to(device)\n        sample_imgs, _, _ = model(imgs)   # (64, 3, 128, 128)\n        # Save a grid of 64 samples as a single image\n        save_image(sample_imgs.cpu(), f\"sample_epoch_test.png\", nrow=8, normalize=True)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}