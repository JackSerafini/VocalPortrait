{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7655818,"sourceType":"datasetVersion","datasetId":4448016}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0ec00c01-bc5f-4755-9863-5a58e48e8c37","cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e1a7e326-d5fe-4acb-9ae4-96f3544c76ce","cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import models\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nfrom torchvision.datasets import ImageFolder, DatasetFolder\nfrom torch.utils.data.dataset import random_split\nfrom PIL import Image\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"034d8be9-2a29-41fb-bba4-650d0f4ed2ec","cell_type":"code","source":"import os\n\nclass VocalPortraitDataset():\n    # TODO: labels not implemented\n    def __init__(self, root_path:str, transform:None):\n        # root_path is the faces folder path\n        self.root_path = root_path\n        self.transform = transform\n        self.image_paths = self.get_image_paths()\n\n    def get_image_paths(self):\n        image_paths = []\n        \n        person_folder = os.listdir(self.root_path)\n        for person in person_folder:\n            for nationality in ['English', 'Urdu']:\n                if os.path.exists(f\"{self.root_path}/{person}/{nationality}\"):\n                    video_ids = os.listdir(f\"{self.root_path}/{person}/{nationality}\")\n                    for video_id in video_ids:\n                        image_names = os.listdir(f\"{self.root_path}/{person}/{nationality}/{video_id}\")\n                        for image_name in image_names:\n                            image_paths.append(f\"{self.root_path}/{person}/{nationality}/{video_id}/{image_name}\")\n        return image_paths\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        # label = self.labels[idx]\n        return image, idx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b74becd0-8a95-4bfd-a3d0-a8f77d5facce","cell_type":"code","source":"class plainVAE(nn.Module):\n    def __init__(self, latent_dim: int = 128):\n        super(plainVAE, self).__init__()\n        self.latent_dim = latent_dim\n\n        # Encoder: input 3x128x128 -> latent mean and log variance\n        self.encoder = nn.Sequential(\n            # Layer 1: 3x128x128 -> 32x64x64 # Should I use bigger images?\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n\n            # Layer 2: 32x64x64 -> 64x32x32 # Should I use bigger images?\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            # Layer 3: 64x32x32 -> 128x16x16\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n\n            # Layer 4: 128x16x16 -> 256x8x8\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n\n            # # Layer 5: 256x8x8 -> 512x4x4\n            # nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            # nn.BatchNorm2d(512),\n            # nn.ReLU(inplace=True),\n        )\n\n        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n\n        self.decoder_fc = nn.Linear(latent_dim, 512 * 4 * 4)\n        self.decoder = nn.Sequential(\n            # Layer 1: 512*4*4 -> 256*8*8\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n\n            # Layer 2: 256*8*8 -> 128*16*16\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            # Layer 3: 128*16*16 -> 64*32*32\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            # # Layer 4: 64*32*32 -> 32*64*64\n            # nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            # nn.BatchNorm2d(32),\n            # nn.ReLU(inplace=True),\n\n            # Layer 4: 32*64*64 -> 3*128*128\n            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = h.view(h.size(0), -1)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n    \n    def reparameterization(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def decode(self, z):\n        h = self.decoder_fc(z)\n        h = h.view(h.size(0), 512, 4, 4)\n        return self.decoder(h)\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterization(mu, logvar)\n        x_reconstructed = self.decode(z)\n        return x_reconstructed, mu, logvar\n\n    def loss(self, x, recon_x, mu, logvar, beta: float = 1.0):\n        recon_loss = F.mse_loss(x, recon_x, reduction='sum') / x.size(0)\n        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n\n        return recon_loss + beta * kl, recon_loss, kl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3a23432d-fc15-4222-abb2-d4bb74d995e3","cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomApply([\n        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n    ], p=0.5),\n    transforms.ToTensor(),         # this ensures values ∈ [0,1]\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dd9cda0a-3bae-44a5-ba99-c1da7b1a4cee","cell_type":"code","source":"# VOICEPORTRAIT dataset\n\ndata_root = \"/kaggle/input/vocalportrait/mavceleb_v1_train_cropped/mavceleb_v1_train_cropped/faces\"\ndataset = VocalPortraitDataset(data_root, transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f72be01b-a590-4a64-8e74-2352a8f12919","cell_type":"code","source":"# CELEBVOICE dataset\n\ndata_root = \"/kaggle/input/cropped-celeba\"\ndataset = ImageFolder(root=data_root, transform=transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cc04b464-ef59-4f51-a527-4c8cb7f2fb26","cell_type":"code","source":"lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\ntrain_dataset, test_dataset = random_split(dataset, lengths)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    pin_memory=True,\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=64,\n    shuffle=True,\n    pin_memory=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3501ef09-08ac-478d-8436-316e9e029761","cell_type":"code","source":"# ----------------------------\n# 3) Instantiate model, optimizer, device\n# ----------------------------\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel = plainVAE(latent_dim=128).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6d31e70f-42a9-4af1-8ef8-dc582331f454","cell_type":"code","source":"# ----------------------------\n# 4) Training loop\n# ----------------------------\nnum_epochs = 5\nlog_interval = 100   # how many batches between printouts\n\nresume = False\ncheckpoint_path = \"epoch{i}.pth\"  # or whichever epoch you want\n\nstart_epoch = 1\nif resume and os.path.exists(checkpoint_path):\n    model.load_state_dict(torch.load(checkpoint_path))\n    print(f\"Resumed model from {checkpoint_path}\")\n    # If you also saved the optimizer state, load it too:\n    # optimizer.load_state_dict(torch.load(\"optimizer_epoch5.pth\"))\n    start_epoch = int(checkpoint_path.split(\"epoch\")[1].split(\".\")[0]) + 1\n\nfor epoch in range(start_epoch, num_epochs + 1):\n    model.train()\n    running_recon_loss = 0.0\n    running_kl_loss = 0.0\n    running_total_loss = 0.0\n\n    for batch_idx, (imgs, _) in enumerate(train_dataloader, start=1):\n        imgs = imgs.to(device)   # shape (B, 3, 128, 128)\n        optimizer.zero_grad()\n\n        recon_imgs, mu, logvar = model(imgs)\n        loss, recon_loss, kl_loss = model.loss(imgs, recon_imgs, mu, logvar, beta=1.0)\n        loss.backward()\n        optimizer.step()\n\n        running_total_loss += loss.item()\n        running_recon_loss += recon_loss.item()\n        running_kl_loss += kl_loss.item()\n\n        if batch_idx % log_interval == 0:\n            avg_total = running_total_loss / log_interval\n            avg_recon = running_recon_loss / log_interval\n            avg_kl    = running_kl_loss / log_interval\n            print(f\"Epoch [{epoch}/{num_epochs}]  \"\n                  f\"Batch [{batch_idx}/{len(train_dataloader)}]  \"\n                  f\"Total Loss: {avg_total:.3f}  \"\n                  f\"Reconstruction: {avg_recon:.3f}  \"\n                  f\"KL: {avg_kl:.3f}\")\n            running_total_loss = 0.0\n            running_recon_loss = 0.0\n            running_kl_loss    = 0.0\n\n    # At the end of each epoch, you can save a checkpoint:\n    checkpoint_path = f\"vae_epoch{epoch}.pth\"\n    torch.save(model.state_dict(), checkpoint_path.format({\"i\": epoch}))\n\n    # (Optional) Also: generate a few samples from N(0,I) and save to disk\n    model.eval()\n    with torch.no_grad():\n        sample_z = torch.randn(64, model.latent_dim).to(device)\n        sample_imgs = model.decode(sample_z)   # (64, 3, 128, 128)\n        # Save a grid of 64 samples as a single image\n        save_image(sample_imgs.cpu(), f\"sample_epoch{epoch}.png\", nrow=8, normalize=True)\n\nprint(\"Training finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"96351251-3419-41bf-955a-daaa7f4672a0","cell_type":"code","source":"model.eval()\nfor batch_idx, (imgs, _) in enumerate(test_dataloader, start=1):\n    with torch.no_grad():\n        imgs = imgs.to(device)\n        sample_imgs, _, _ = model(imgs)   # (64, 3, 128, 128)\n        # Save a grid of 64 samples as a single image\n        save_image(sample_imgs.cpu(), f\"sample_epoch_test.png\", nrow=8, normalize=True)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}