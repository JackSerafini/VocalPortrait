{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a306ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import Optional, Callable, Tuple\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plainVAE(nn.Module):\n",
    "    def __init__(self, latent_dim: int = 128):\n",
    "        super(plainVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: input 3x128x128 -> latent mean and log variance\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Layer 1: 3x128x128 -> 32x64x64 # Should I use bigger images?\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Layer 2: 32x64x64 -> 64x32x32 # Should I use bigger images?\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Layer 3: 64x32x32 -> 128x16x16\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Layer 4: 128x16x16 -> 256x8x8\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # # Layer 5: 256x8x8 -> 512x4x4\n",
    "            # nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(512),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 512 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Layer 1: 512*4*4 -> 256*8*8\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Layer 2: 256*8*8 -> 128*16*16\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Layer 3: 128*16*16 -> 64*32*32\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # # Layer 4: 64*32*32 -> 32*64*64\n",
    "            # nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "\n",
    "            # Layer 4: 32*64*64 -> 3*128*128\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterization(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.decoder_fc(z)\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        return self.decoder(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mu, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "    def loss(self, x, recon_x, mu, logvar, beta: float = 1.0):\n",
    "        recon_loss = F.mse_loss(x, recon_x, reduction='sum') / x.size(0)\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "\n",
    "        return recon_loss + beta * kl, recon_loss, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8de256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalPortraitDataset:\n",
    "    def __init__(self, root_path: str, transform: Optional[Callable] = None):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "        self.samples = self._build_samples()  # List of (image_path, label) tuples\n",
    "        \n",
    "    def _build_samples(self) -> list[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            List of tuples containing (image_path, label) where label is the\n",
    "            relative path from person directory (e.g. \"person001/English/source001/face001.jpg\")\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        for root, _, files in os.walk(self.root_path):\n",
    "            for file in files:\n",
    "\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    rel_path = os.path.relpath(full_path, start=self.root_path)\n",
    "                    samples.append((full_path, rel_path))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - The transformed image tensor\n",
    "            - The label string (relative path from person directory)\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c26e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    ], p=0.5),\n",
    "    transforms.ToTensor(),         # this ensures values âˆˆ [0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42222214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle the labels properly\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack images\n",
    "    labels = [item[1] for item in batch]               # Keep labels as a list\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "dataset = VocalPortraitDataset(root_path=\"data/mavceleb_train/faces\", transform=transform)\n",
    "lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
    "train_dataset, test_dataset = random_split(dataset, lengths)\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d50e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,  # Handles labels\n",
    "    num_workers=4,\n",
    "    pin_memory=True,  # Faster GPU transfer if using CUDA\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc847490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Instantiate model, optimizer, device\n",
    "# ----------------------------\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model = plainVAE(latent_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) Training loop\n",
    "# ----------------------------\n",
    "num_epochs = 1\n",
    "log_interval = 100\n",
    "\n",
    "# Checkpoint configuration\n",
    "resume = True\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize CSV storage for mu, logvar\n",
    "train_latent_data = []\n",
    "\n",
    "# Resume logic\n",
    "start_epoch = 1\n",
    "if resume:\n",
    "    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith(\"vae_epoch\")])\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
    "        model.load_state_dict(torch.load(latest_checkpoint))\n",
    "        print(f\"Resumed model from {latest_checkpoint}\")\n",
    "        start_epoch = int(latest_checkpoint.split(\"epoch\")[1].split(\".\")[0]) + 1\n",
    "        # To resume optimizer state (if saved):\n",
    "        # optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, f\"optimizer_epoch{start_epoch-1}.pth\")))\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_recon_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (imgs, labels) in enumerate(train_dataloader, start=1):\n",
    "        imgs = imgs.to(device)   # shape (B, 3, 128, 128)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_imgs, mu, logvar = model(imgs)\n",
    "        loss, recon_loss, kl_loss = model.loss(imgs, recon_imgs, mu, logvar, beta=1.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store training latent variables for last epoch only\n",
    "        if epoch == num_epochs:\n",
    "            for label, m, lv in zip(labels, mu, logvar):\n",
    "                train_latent_data.append({\n",
    "                    \"label\": label,\n",
    "                    \"mu\": m.cpu().detach().numpy(),\n",
    "                    \"logvar\": lv.cpu().detach().numpy()\n",
    "                })\n",
    "\n",
    "        running_total_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_kl_loss += kl_loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            avg_total = running_total_loss / log_interval\n",
    "            avg_recon = running_recon_loss / log_interval\n",
    "            avg_kl    = running_kl_loss / log_interval\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}]  \"\n",
    "                  f\"Batch [{batch_idx}/{len(train_dataloader)}]  \"\n",
    "                  f\"Total Loss: {avg_total:.3f}  \"\n",
    "                  f\"Reconstruction: {avg_recon:.3f}  \"\n",
    "                  f\"KL: {avg_kl:.3f}    \"\n",
    "                  f\"KL: {len(mu), len(logvar)}\")\n",
    "            running_total_loss = 0.0\n",
    "            running_recon_loss = 0.0\n",
    "            running_kl_loss    = 0.0\n",
    "    \n",
    "    # Save checkpoints\n",
    "    '''model_checkpoint = os.path.join(checkpoint_dir, f\"vae_epoch{epoch}.pth\")\n",
    "    optimizer_checkpoint = os.path.join(checkpoint_dir, f\"optimizer_epoch{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), model_checkpoint)\n",
    "    torch.save(optimizer.state_dict(), optimizer_checkpoint)\n",
    "    print(f\"Saved checkpoint at epoch {epoch}\")'''\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b02ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mu, logvar to csv\n",
    "def save_latent_data(data, filename):\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert numpy arrays to string representation for CSV\n",
    "    df['mu'] = df['mu'].apply(lambda x: np.array2string(x, separator=',', threshold=np.inf))\n",
    "    df['logvar'] = df['logvar'].apply(lambda x: np.array2string(x, separator=',', threshold=np.inf))\n",
    "    \n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved latent variables to {filename}\")\n",
    "\n",
    "save_latent_data(train_latent_data, \"train_latent_variables.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68cf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_idx, (imgs, _) in enumerate(test_dataloader, start=1):\n",
    "    with torch.no_grad():\n",
    "        imgs = imgs.to(device)\n",
    "        sample_imgs, _, _ = model(imgs)   # (64, 3, 128, 128)\n",
    "        # Save a grid of 64 samples as a single image\n",
    "        save_image(sample_imgs.cpu(), f\"sample_epoch_test.png\", nrow=8, normalize=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85132c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class VocalPortraitDataset():\n",
    "    # TODO: labels not implemented\n",
    "    def __init__(self, root_path:str, transform:None):\n",
    "        # root_path is the faces folder path\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "        self.image_paths = self.get_image_paths()\n",
    "\n",
    "    def get_image_paths(self):\n",
    "        image_paths = []\n",
    "        \n",
    "        person_folder = os.listdir(self.root_path)\n",
    "        for person in person_folder:\n",
    "            for nationality in ['English', 'Urdu']:\n",
    "                if os.path.exists(f\"{self.root_path}/{person}/{nationality}\"):\n",
    "                    video_ids = os.listdir(f\"{self.root_path}/{person}/{nationality}\")\n",
    "                    for video_id in video_ids:\n",
    "                        image_names = os.listdir(f\"{self.root_path}/{person}/{nationality}/{video_id}\")\n",
    "                        for image_name in image_names:\n",
    "                            image_paths.append(f\"{self.root_path}/{person}/{nationality}/{video_id}/{image_name}\")\n",
    "        return image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #Â label = self.labels[idx]\n",
    "        return image, idx\n",
    "    \n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/vocalportrait/mavceleb_v1_train_cropped/mavceleb_v1_train_cropped/faces\"\n",
    "dataset = VocalPortraitDataset(data_root, transform)\n",
    "\n",
    "lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
    "train_dataset, test_dataset = random_split(dataset, lengths)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# ----------------------------\n",
    "# 3) Instantiate model, optimizer, device\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = plainVAE(latent_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Training loop\n",
    "# ----------------------------\n",
    "num_epochs = 5\n",
    "log_interval = 100   # how many batches between printouts\n",
    "\n",
    "resume = True\n",
    "checkpoint_path = \"/kaggle/input/vae8epoch/pytorch/default/1/vae_epoch8.pth\"  # or whichever epoch you want\n",
    "\n",
    "start_epoch = 1\n",
    "if resume and os.path.exists(checkpoint_path):\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Resumed model from {checkpoint_path}\")\n",
    "    # If you also saved the optimizer state, load it too:\n",
    "    # optimizer.load_state_dict(torch.load(\"optimizer_epoch5.pth\"))\n",
    "    # start_epoch = int(checkpoint_path.split(\"epoch\")[1].split(\".\")[0]) + 1\n",
    "    start_epoch = 9\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_recon_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (imgs, _) in enumerate(train_dataloader, start=1):\n",
    "        imgs = imgs.to(device)   # shape (B, 3, 128, 128)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_imgs, mu, logvar = model(imgs)\n",
    "        loss, recon_loss, kl_loss = model.loss(imgs, recon_imgs, mu, logvar, beta=1.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_total_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_kl_loss += kl_loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            avg_total = running_total_loss / log_interval\n",
    "            avg_recon = running_recon_loss / log_interval\n",
    "            avg_kl    = running_kl_loss / log_interval\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}]  \"\n",
    "                  f\"Batch [{batch_idx}/{len(train_dataloader)}]  \"\n",
    "                  f\"Total Loss: {avg_total:.3f}  \"\n",
    "                  f\"Reconstruction: {avg_recon:.3f}  \"\n",
    "                  f\"KL: {avg_kl:.3f}\")\n",
    "            running_total_loss = 0.0\n",
    "            running_recon_loss = 0.0\n",
    "            running_kl_loss    = 0.0\n",
    "\n",
    "    # At the end of each epoch, you can save a checkpoint:\n",
    "    checkpoint_path = f\"vae_epoch{epoch}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path.format({\"i\": epoch}))\n",
    "\n",
    "    # (Optional) Also: generate a few samples from N(0,I) and save to disk\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_z = torch.randn(64, model.latent_dim).to(device)\n",
    "        sample_imgs = model.decode(sample_z)   # (64, 3, 128, 128)\n",
    "        # Save a grid of 64 samples as a single image\n",
    "        save_image(sample_imgs.cpu(), f\"sample_epoch{epoch}.png\", nrow=8, normalize=True)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for batch_idx, (imgs, _) in enumerate(test_dataloader, start=1):\n",
    "    with torch.no_grad():\n",
    "        imgs = imgs.to(device)\n",
    "        sample_imgs, _, _ = model(imgs)   # (64, 3, 128, 128)\n",
    "        # Save a grid of 64 samples as a single image\n",
    "        save_image(sample_imgs.cpu(), f\"sample_epoch_test.png\", nrow=8, normalize=True)\n",
    "    break'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
